{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f37491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d63853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- 1. Research Configuration ---\n",
    "class Config:\n",
    "    # Model Settings\n",
    "    SD_MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "    CLIP_MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "    # Output Settings\n",
    "    BASE_DIR = \"/home/alex/data/synthetic_cifar100_research\"\n",
    "    # We save HIGH_RES to test if DINOv2 fails due to resolution or domain shift\n",
    "    SAVE_HIGH_RES = True\n",
    "    TARGET_RES = (32, 32)\n",
    "\n",
    "    # Generation Settings\n",
    "    IMAGES_PER_CLASS = 100      # Final count needed\n",
    "    OVERSAMPLE_FACTOR = 2.0    # Generate 2x needed, keep top 50% via CLIP\n",
    "    BATCH_SIZE = 5             # Adjust based on GPU VRAM\n",
    "    SEED = 42\n",
    "\n",
    "    # Inference Parameters\n",
    "    STEPS = 50                 # Increased from 30 for higher fidelity\n",
    "    GUIDANCE = 7.5\n",
    "\n",
    "# --- 2. Semantics & Prompt Engineering ---\n",
    "\n",
    "# Ensemble templates to prevent \"pose bias\" (e.g., all cars facing left)\n",
    "PROMPT_TEMPLATES = [\n",
    "    \"a photo of a {}.\",\n",
    "    \"a close-up photo of the {}.\",\n",
    "    \"a bright photo of a {}.\",\n",
    "    \"a cropped photo of the {}.\",\n",
    "    \"a pixelated photo of a {}.\"\n",
    "]\n",
    "\n",
    "# Disambiguation: Map CIFAR labels to semantically specific prompts\n",
    "CLASS_PROMPT_MAP = {\n",
    "    'apple': 'red apple fruit',\n",
    "    'aquarium_fish': 'aquarium fish in a tank',\n",
    "    'baby': 'human baby',\n",
    "    'bear': 'brown bear',\n",
    "    'beaver': 'beaver animal',\n",
    "    'bed': 'bedroom bed furniture',\n",
    "    'bee': 'honey bee insect',\n",
    "    'beetle': 'beetle insect',\n",
    "    'bicycle': 'bicycle',\n",
    "    'bottle': 'glass bottle',\n",
    "    'bowl': 'kitchen bowl',\n",
    "    'boy': 'young boy',\n",
    "    'bridge': 'architectural bridge',\n",
    "    'bus': 'public transit bus',\n",
    "    'butterfly': 'butterfly insect',\n",
    "    'camel': 'camel animal',\n",
    "    'can': 'metal beverage can',  # Crucial fix for polysemy\n",
    "    'castle': 'castle building',\n",
    "    'caterpillar': 'caterpillar insect',\n",
    "    'cattle': 'cow cattle',\n",
    "    'chair': 'furniture chair',\n",
    "    'chimpanzee': 'chimpanzee ape',\n",
    "    'clock': 'analog wall clock',\n",
    "    'cloud': 'sky cloud',\n",
    "    'cockroach': 'cockroach insect',\n",
    "    'couch': 'living room couch sofa',\n",
    "    'crab': 'crab crustacean on beach',\n",
    "    'crocodile': 'crocodile reptile',\n",
    "    'cup': 'drinking cup',\n",
    "    'dinosaur': 'dinosaur',\n",
    "    'dolphin': 'dolphin sea mammal',\n",
    "    'elephant': 'elephant',\n",
    "    'flatfish': 'flatfish flounder underwater',\n",
    "    'forest': 'forest landscape',\n",
    "    'fox': 'fox animal',\n",
    "    'girl': 'young girl',\n",
    "    'hamster': 'hamster animal',\n",
    "    'house': 'residential house building',\n",
    "    'kangaroo': 'kangaroo animal',\n",
    "    'keyboard': 'computer keyboard',\n",
    "    'lamp': 'table lamp',\n",
    "    'lawn_mower': 'lawn mower machine',\n",
    "    'leopard': 'leopard big cat',\n",
    "    'lion': 'lion big cat',\n",
    "    'lizard': 'lizard reptile',\n",
    "    'lobster': 'lobster crustacean',\n",
    "    'man': 'adult man',\n",
    "    'maple_tree': 'maple tree',\n",
    "    'motorcycle': 'motorcycle vehicle',\n",
    "    'mountain': 'mountain landscape',\n",
    "    'mouse': 'mouse animal', # Disambiguate from computer mouse\n",
    "    'mushroom': 'mushroom fungus',\n",
    "    'oak_tree': 'oak tree',\n",
    "    'orange': 'orange fruit',\n",
    "    'orchid': 'orchid flower',\n",
    "    'otter': 'otter animal',\n",
    "    'palm_tree': 'palm tree',\n",
    "    'pear': 'pear fruit',\n",
    "    'pickup_truck': 'pickup truck vehicle',\n",
    "    'pine_tree': 'pine tree',\n",
    "    'plain': 'grassy plains landscape', # Fix: Avoids geometry planes\n",
    "    'plate': 'dinner plate',\n",
    "    'poppy': 'poppy flower',\n",
    "    'porcupine': 'porcupine animal',\n",
    "    'possum': 'possum animal',\n",
    "    'rabbit': 'rabbit animal',\n",
    "    'raccoon': 'raccoon animal',\n",
    "    'ray': 'stingray fish underwater', # Fix: Avoids light rays\n",
    "    'road': 'asphalt road',\n",
    "    'rocket': 'space rocket launch',\n",
    "    'rose': 'rose flower',\n",
    "    'sea': 'ocean sea landscape',\n",
    "    'seal': 'seal animal',\n",
    "    'shark': 'shark fish',\n",
    "    'shrew': 'shrew animal',\n",
    "    'skunk': 'skunk animal',\n",
    "    'skyscraper': 'skyscraper building',\n",
    "    'snail': 'snail mollusk',\n",
    "    'snake': 'snake reptile',\n",
    "    'spider': 'spider insect',\n",
    "    'squirrel': 'squirrel animal',\n",
    "    'streetcar': 'streetcar tram',\n",
    "    'sunflower': 'sunflower',\n",
    "    'sweet_pepper': 'sweet pepper vegetable',\n",
    "    'table': 'wooden dining table',\n",
    "    'tank': 'military tank vehicle',\n",
    "    'telephone': 'rotary telephone',\n",
    "    'television': 'television set',\n",
    "    'tiger': 'tiger big cat',\n",
    "    'tractor': 'farm tractor',\n",
    "    'train': 'locomotive train',\n",
    "    'trout': 'trout fish',\n",
    "    'tulip': 'tulip flower',\n",
    "    'turtle': 'turtle reptile',\n",
    "    'wardrobe': 'wardrobe closet furniture',\n",
    "    'whale': 'whale sea mammal',\n",
    "    'willow_tree': 'weeping willow tree',\n",
    "    'wolf': 'wolf animal',\n",
    "    'woman': 'adult woman',\n",
    "    'worm': 'earthworm'\n",
    "}\n",
    "\n",
    "# --- 3. Pipeline Setup ---\n",
    "\n",
    "def setup_pipelines(device):\n",
    "    \"\"\"Loads Generative (SD) and Discriminative (CLIP) models.\"\"\"\n",
    "    print(\"Loading Stable Diffusion...\")\n",
    "    sd_pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        Config.SD_MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True\n",
    "    )\n",
    "    sd_pipe.to(device)\n",
    "    sd_pipe.set_progress_bar_config(disable=True)\n",
    "\n",
    "    print(\"Loading CLIP for Quality Control...\")\n",
    "    clip_model = CLIPModel.from_pretrained(\n",
    "        Config.CLIP_MODEL_ID,\n",
    "        use_safetensors=True\n",
    "    ).to(device)\n",
    "    clip_processor = CLIPProcessor.from_pretrained(Config.CLIP_MODEL_ID)\n",
    "\n",
    "    return sd_pipe, clip_model, clip_processor\n",
    "\n",
    "def score_images(images, prompt, model, processor, device):\n",
    "    \"\"\"Returns CLIP similarity scores for a batch of images against the prompt.\"\"\"\n",
    "    inputs = processor(text=[prompt], images=images, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Image-Text similarity score\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=0) # normalize\n",
    "\n",
    "    return probs.cpu().numpy().flatten()\n",
    "\n",
    "# --- 4. Main Loop ---\n",
    "\n",
    "def generate_synthetic_cifar():\n",
    "    logging.basicConfig(filename='generation_log.txt', level=logging.INFO)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Initialize\n",
    "    sd_pipe, clip_model, clip_processor = setup_pipelines(device)\n",
    "    generator = torch.Generator(device).manual_seed(Config.SEED)\n",
    "\n",
    "    # Calculate totals\n",
    "    total_needed = int(Config.IMAGES_PER_CLASS * Config.OVERSAMPLE_FACTOR)\n",
    "\n",
    "    print(f\"Goal: {Config.IMAGES_PER_CLASS} high-quality images per class.\")\n",
    "    print(f\"Strategy: Generate {total_needed}, filter with CLIP, keep top {Config.IMAGES_PER_CLASS}.\")\n",
    "\n",
    "    for class_name, specific_prompt in tqdm(CLASS_PROMPT_MAP.items(), desc=\"Classes\"):\n",
    "\n",
    "        # Directory Setup\n",
    "        print(\"Setting up directories...\")\n",
    "        dir_32 = os.path.join(Config.BASE_DIR, \"cifar100_32x32\", class_name)\n",
    "        dir_512 = os.path.join(Config.BASE_DIR, \"cifar100_512x512_master\", class_name)\n",
    "        os.makedirs(dir_32, exist_ok=True)\n",
    "        if Config.SAVE_HIGH_RES:\n",
    "            os.makedirs(dir_512, exist_ok=True)\n",
    "\n",
    "        # Check existing (Resume capability)\n",
    "        print(\"checking existing counts\")\n",
    "        existing_count = len([f for f in os.listdir(dir_32) if f.endswith('.png')])\n",
    "        if existing_count >= Config.IMAGES_PER_CLASS:\n",
    "            continue\n",
    "\n",
    "        # Candidate Storage\n",
    "        candidates = [] # Stores (image, score) tuples\n",
    "\n",
    "        # Generation Loop (Oversample)\n",
    "        print(\"Beginning generation loop\")\n",
    "        with tqdm(total=total_needed, desc=f\"Gen: {class_name}\", leave=False) as pbar:\n",
    "            while len(candidates) < total_needed:\n",
    "                # Randomly select a template for variety\n",
    "                template = np.random.choice(PROMPT_TEMPLATES)\n",
    "                prompt_text = template.format(specific_prompt)\n",
    "\n",
    "                # Batch Generation\n",
    "                current_batch_size = min(Config.BATCH_SIZE, total_needed - len(candidates))\n",
    "\n",
    "                with torch.autocast(device):\n",
    "                    images = sd_pipe(\n",
    "                        [prompt_text] * current_batch_size,\n",
    "                        num_inference_steps=Config.STEPS,\n",
    "                        guidance_scale=Config.GUIDANCE,\n",
    "                        generator=generator,\n",
    "                    ).images\n",
    "\n",
    "                # Quality Control (CLIP Scoring)\n",
    "                scores = score_images(images, specific_prompt, clip_model, clip_processor, device)\n",
    "\n",
    "                # Store Candidates\n",
    "                for img, score in zip(images, scores):\n",
    "                    candidates.append((img, score))\n",
    "\n",
    "                pbar.update(current_batch_size)\n",
    "\n",
    "        # Filtering: Sort by CLIP score and keep the best\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        best_candidates = candidates[:Config.IMAGES_PER_CLASS]\n",
    "\n",
    "        # Save Phase\n",
    "        for idx, (img, score) in enumerate(best_candidates):\n",
    "            filename = f\"{class_name}_{idx:03d}.png\"\n",
    "\n",
    "            # 1. Save Master (512x512) for Ablation Studies\n",
    "            if Config.SAVE_HIGH_RES:\n",
    "                img.save(os.path.join(dir_512, filename))\n",
    "\n",
    "            # 2. Save Target (32x32) for Main Experiment\n",
    "            img_resized = img.resize(Config.TARGET_RES, Image.LANCZOS)\n",
    "            img_resized.save(os.path.join(dir_32, filename))\n",
    "\n",
    "            # 3. Log Metadata\n",
    "            logging.info(json.dumps({\n",
    "                \"class\": class_name,\n",
    "                \"filename\": filename,\n",
    "                \"clip_score\": float(score),\n",
    "                \"original_prompt\": specific_prompt\n",
    "            }))\n",
    "\n",
    "    print(f\"Dataset generation complete. Saved to {Config.BASE_DIR}\")\n",
    "\n",
    "generate_synthetic_cifar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
